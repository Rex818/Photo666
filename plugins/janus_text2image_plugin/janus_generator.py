"""
åŸºäºå®˜æ–¹ deepseek-ai/Janus ä»“åº“çš„å›¾åƒç”Ÿæˆå™¨
ç›´æ¥ä½¿ç”¨å®˜æ–¹å®ç°ä»£ç 
"""
import torch
import numpy as np
from PIL import Image
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)


class JanusImageGenerator:
    """Janus Pro å›¾åƒç”Ÿæˆå™¨ - åŸºäºå®˜æ–¹GitHubå®ç°"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = None
        
    def load_model(self, model, processor):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        self.model = model
        self.processor = processor
        self.device = next(model.parameters()).device
        logger.info(f"Janus Pro generator loaded on device: {self.device}")
        
    def generate_images(self, 
                       prompt: str, 
                       seed: int = 42,
                       batch_size: int = 1, 
                       temperature: float = 1.0, 
                       cfg_weight: float = 5.0, 
                       top_p: float = 0.95,
                       img_size: int = 384,
                       max_batch_size: int = 4) -> List[np.ndarray]:
        """
        åŸºäºå®˜æ–¹GitHubä»“åº“çš„å›¾åƒç”Ÿæˆå®ç°
        å‚è€ƒ: https://github.com/deepseek-ai/Janus
        """
        if self.model is None or self.processor is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
            
        try:
            import gc
            
            # æ˜¾å­˜ç®¡ç†
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                logger.info(f"âœ… ä½¿ç”¨GPUç”Ÿæˆ: {torch.cuda.get_device_name(0)}")
            
            # è®¾ç½®éšæœºç§å­
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
            
            logger.info(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ: '{prompt}'")
            
            # è¯¦ç»†è°ƒè¯•å®˜æ–¹ç”Ÿæˆæ–¹æ³•
            try:
                logger.info("=== å¼€å§‹è¯¦ç»†è°ƒè¯•å®˜æ–¹ç”Ÿæˆæ–¹æ³• ===")
                
                # æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹å’Œprocessorçš„å±æ€§
                logger.info(f"æ¨¡å‹ç±»å‹: {type(self.model)}")
                logger.info(f"å¤„ç†å™¨ç±»å‹: {type(self.processor)}")
                logger.info(f"æ¨¡å‹æ˜¯å¦æœ‰generateæ–¹æ³•: {hasattr(self.model, 'generate')}")
                logger.info(f"æ¨¡å‹æ˜¯å¦æœ‰gen_vision_model: {hasattr(self.model, 'gen_vision_model')}")
                
                # æ­¥éª¤2: å‡†å¤‡å¯¹è¯æ ¼å¼
                conversation = [
                    {"role": "<|User|>", "content": prompt},
                    {"role": "<|Assistant|>", "content": ""},
                ]
                logger.info(f"å¯¹è¯æ ¼å¼: {conversation}")
                
                # æ­¥éª¤3: åº”ç”¨SFTæ¨¡æ¿
                sft_format = self.processor.apply_sft_template_for_multi_turn_prompts(
                    conversations=conversation,
                    sft_format=self.processor.sft_format,
                    system_prompt="",
                )
                logger.info(f"SFTæ ¼å¼åŒ–ç»“æœé•¿åº¦: {len(sft_format)}")
                logger.info(f"SFTæ ¼å¼åŒ–ç»“æœå‰100å­—ç¬¦: {sft_format[:100]}")
                
                # æ­¥éª¤4: æ·»åŠ å›¾åƒå¼€å§‹æ ‡è®°
                logger.info(f"å›¾åƒå¼€å§‹æ ‡è®°: {repr(self.processor.image_start_tag)}")
                prompt_text = sft_format + self.processor.image_start_tag
                logger.info(f"å®Œæ•´æç¤ºè¯é•¿åº¦: {len(prompt_text)}")
                
                # æ­¥éª¤5: ç¼–ç è¾“å…¥
                input_ids = self.processor.tokenizer.encode(prompt_text)
                input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(self.device)
                logger.info(f"è¾“å…¥tokenå½¢çŠ¶: {input_ids.shape}")
                logger.info(f"è¾“å…¥tokenå‰10ä¸ª: {input_ids[0, :10].tolist()}")
                
                # æ­¥éª¤6: ä½¿ç”¨æ­£ç¡®çš„Janusç”Ÿæˆæ–¹æ³•
                logger.info("æ¨¡å‹æ²¡æœ‰generateæ–¹æ³•ï¼Œä½¿ç”¨Janusä¸“ç”¨çš„ç”Ÿæˆæ–¹å¼...")
                
                # æ£€æŸ¥æ¨¡å‹çš„å®é™…æ–¹æ³•
                model_methods = [method for method in dir(self.model) if not method.startswith('_')]
                logger.info(f"æ¨¡å‹å¯ç”¨æ–¹æ³•: {model_methods[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                
                # å°è¯•ä½¿ç”¨language_modelçš„generateæ–¹æ³•
                if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'generate'):
                    logger.info("ä½¿ç”¨language_model.generateæ–¹æ³•...")
                    
                    generation_config = {
                        "max_new_tokens": 576,
                        "temperature": temperature,
                        "do_sample": True,
                        "top_p": top_p,
                        "pad_token_id": self.processor.tokenizer.eos_token_id,
                        "eos_token_id": self.processor.tokenizer.eos_token_id,
                    }
                    logger.info(f"ç”Ÿæˆé…ç½®: {generation_config}")
                    
                    with torch.no_grad():
                        outputs = self.model.language_model.generate(input_ids, **generation_config)
                        logger.info(f"ç”Ÿæˆè¾“å‡ºå½¢çŠ¶: {outputs.shape}")
                        
                elif hasattr(self.processor, 'generate'):
                    logger.info("ä½¿ç”¨processor.generateæ–¹æ³•...")
                    
                    with torch.no_grad():
                        outputs = self.processor.generate(
                            self.model,
                            prompt_text,
                            max_new_tokens=576,
                            temperature=temperature,
                            do_sample=True,
                            top_p=top_p,
                        )
                        logger.info(f"processorç”Ÿæˆè¾“å‡º: {type(outputs)}")
                        
                        # å¦‚æœprocessorè¿”å›çš„æ˜¯å›¾åƒï¼Œç›´æ¥ä½¿ç”¨
                        if hasattr(outputs, 'images') and outputs.images is not None:
                            decoded_images = outputs.images
                            logger.info(f"ç›´æ¥ä»processorè·å–å›¾åƒ: {decoded_images.shape}")
                        else:
                            # å¦åˆ™æå–tokens
                            if hasattr(outputs, 'sequences'):
                                generated_tokens = outputs.sequences[0, input_ids.shape[1]:]
                            else:
                                generated_tokens = outputs[0, input_ids.shape[1]:]
                            logger.info(f"ä»processoræå–tokens: {len(generated_tokens)}")
                            
                            # ç»§ç»­åç»­å¤„ç†
                            if len(generated_tokens) < 576:
                                padding_needed = 576 - len(generated_tokens)
                                padding = torch.zeros(padding_needed, dtype=generated_tokens.dtype, device=generated_tokens.device)
                                generated_tokens = torch.cat([generated_tokens, padding])
                            
                            image_tokens = generated_tokens[:576].view(1, 24, 24)
                            decoded_images = self.model.gen_vision_model.decode_code(image_tokens.to(dtype=torch.int))
                            
                else:
                    raise Exception("æ‰¾ä¸åˆ°åˆé€‚çš„ç”Ÿæˆæ–¹æ³•")
                
                # æ­¥éª¤8: æå–å›¾åƒtokens
                generated_tokens = outputs[0, input_ids.shape[1]:]
                logger.info(f"ç”Ÿæˆçš„å›¾åƒtokensæ•°é‡: {len(generated_tokens)}")
                logger.info(f"ç”Ÿæˆçš„tokenså‰10ä¸ª: {generated_tokens[:10].tolist()}")
                logger.info(f"ç”Ÿæˆçš„tokenså10ä¸ª: {generated_tokens[-10:].tolist()}")
                
                # æ­¥éª¤9: æ£€æŸ¥tokensæ•°é‡
                if len(generated_tokens) == 0:
                    raise Exception("æ²¡æœ‰ç”Ÿæˆä»»ä½•å›¾åƒtokens")
                elif len(generated_tokens) < 576:
                    logger.warning(f"ç”Ÿæˆçš„tokensä¸è¶³: {len(generated_tokens)} < 576ï¼Œè¿›è¡Œè¡¥é½")
                    padding_needed = 576 - len(generated_tokens)
                    padding = torch.zeros(padding_needed, dtype=generated_tokens.dtype, device=generated_tokens.device)
                    generated_tokens = torch.cat([generated_tokens, padding])
                    logger.info(f"è¡¥é½åçš„tokensæ•°é‡: {len(generated_tokens)}")
                
                # æ­¥éª¤10: é‡å¡‘tokens
                image_tokens = generated_tokens[:576].view(1, 24, 24)
                logger.info(f"é‡å¡‘åçš„å›¾åƒtokenså½¢çŠ¶: {image_tokens.shape}")
                logger.info(f"tokenså€¼èŒƒå›´: [{image_tokens.min().item()}, {image_tokens.max().item()}]")
                
                # æ­¥éª¤11: æ£€æŸ¥è§£ç å™¨
                if not hasattr(self.model, 'gen_vision_model'):
                    raise Exception("æ¨¡å‹æ²¡æœ‰gen_vision_modelå±æ€§")
                
                if not hasattr(self.model.gen_vision_model, 'decode_code'):
                    raise Exception("gen_vision_modelæ²¡æœ‰decode_codeæ–¹æ³•")
                
                # æ­¥éª¤12: è§£ç å›¾åƒ
                logger.info("å¼€å§‹è§£ç å›¾åƒ...")
                decoded_images = self.model.gen_vision_model.decode_code(
                    image_tokens.to(dtype=torch.int)
                )
                logger.info(f"è§£ç æˆåŠŸï¼å›¾åƒå½¢çŠ¶: {decoded_images.shape}")
                logger.info(f"å›¾åƒå€¼èŒƒå›´: [{decoded_images.min().item()}, {decoded_images.max().item()}]")
                
                # æ£€æŸ¥è§£ç ç»“æœæ˜¯å¦åˆç†
                if decoded_images.shape[0] != batch_size:
                    logger.warning(f"æ‰¹é‡å¤§å°ä¸åŒ¹é…: æœŸæœ›{batch_size}, å®é™…{decoded_images.shape[0]}")
                
                if decoded_images.shape[1] != 3:
                    logger.warning(f"é€šé“æ•°ä¸åŒ¹é…: æœŸæœ›3, å®é™…{decoded_images.shape[1]}")
                
                logger.info("=== å®˜æ–¹ç”Ÿæˆæ–¹æ³•è°ƒè¯•å®Œæˆ ===")
                
            except Exception as e:
                logger.error(f"=== å®˜æ–¹æ–¹æ³•åœ¨æ­¥éª¤ä¸­å¤±è´¥ ===")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {e}")
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                logger.info("ä½¿ç”¨fallbackæµ‹è¯•å›¾åƒ...")
                decoded_images = self._create_test_image(batch_size, img_size)
            
            # åå¤„ç†å›¾åƒ (å®˜æ–¹æ–¹å¼)
            images_np = decoded_images.to(torch.float32).cpu().numpy()
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ ‡å‡†åŒ–åˆ°[0,1]èŒƒå›´
            images_np = (images_np + 1.0) / 2.0
            images_np = np.clip(images_np, 0, 1)
            
            # è½¬æ¢æ ¼å¼ BCHW -> BHWC
            if len(images_np.shape) == 4 and images_np.shape[1] == 3:
                images_np = np.transpose(images_np, (0, 2, 3, 1))
            
            # è½¬æ¢ä¸º0-255èŒƒå›´
            images_np = (images_np * 255).astype(np.uint8)
            
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(images_np)} å¼ å›¾åƒï¼Œå½¢çŠ¶: {images_np[0].shape}")
            
            return images_np
            
        except Exception as e:
            logger.error(f"å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å…·ä½“çš„é”™è¯¯ä¿¡æ¯
            error_msg = str(e)
            if "CUDA out of memory" in error_msg:
                raise RuntimeError(f"æ˜¾å­˜ä¸è¶³: {e}\nå»ºè®®ï¼šå‡å°‘æ‰¹é‡å¤§å°æˆ–å›¾åƒå°ºå¯¸")
            elif "shape" in error_msg.lower() or "size" in error_msg.lower():
                raise RuntimeError(f"å¼ é‡å½¢çŠ¶é”™è¯¯: {e}\nå¯èƒ½æ˜¯æ¨¡å‹ä¸å…¼å®¹æˆ–å‚æ•°è®¾ç½®æœ‰è¯¯")
            elif "attribute" in error_msg.lower():
                raise RuntimeError(f"æ¨¡å‹æ¥å£é”™è¯¯: {e}\nå¯èƒ½æ˜¯janusåº“ç‰ˆæœ¬ä¸å…¼å®¹")
            else:
                raise RuntimeError(f"å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
    
    def _decode_generated_tokens(self, generated_tokens, batch_size):
        """è§£ç ç”Ÿæˆçš„tokensä¸ºå›¾åƒ"""
        try:
            logger.info("å¼€å§‹tokenè§£ç ...")
            logger.info(f"Generated tokens shape: {generated_tokens.shape}")
            
            # æ ¹æ®é”™è¯¯ä¿¡æ¯ï¼Œæ¨¡å‹æœŸæœ›è¾“å…¥æœ‰8ä¸ªé€šé“
            # 576 tokens éœ€è¦é‡æ–°åˆ†é…åˆ°åˆé€‚çš„ç»´åº¦
            
            # å°è¯•ä¸åŒçš„reshapeæ–¹å¼
            decode_methods = [
                # æ–¹æ³•1: [batch, 8, 9, 8] - 8*9*8=576
                (8, 9, 8),
                # æ–¹æ³•2: [batch, 9, 8, 8] - 9*8*8=576  
                (9, 8, 8),
                # æ–¹æ³•3: [batch, 6, 12, 8] - 6*12*8=576
                (6, 12, 8),
                # æ–¹æ³•4: [batch, 12, 6, 8] - 12*6*8=576
                (12, 6, 8),
                # æ–¹æ³•5: [batch, 72, 8] - 72*8=576 (3D)
                (72, 8),
                # æ–¹æ³•6: [batch, 24, 24] - 24*24=576 (åŸå§‹æ–¹å¼)
                (24, 24),
            ]
            
            for i, shape in enumerate(decode_methods, 1):
                try:
                    if len(shape) == 3:  # 4D reshape
                        h, w, c = shape
                        tokens_reshaped = generated_tokens.view(batch_size, h, w, c)
                        logger.info(f"æ–¹æ³•{i}: å°è¯•4D reshape {tokens_reshaped.shape}")
                    elif len(shape) == 2:  # 3D reshape
                        h, w = shape
                        tokens_reshaped = generated_tokens.view(batch_size, h, w)
                        logger.info(f"æ–¹æ³•{i}: å°è¯•3D reshape {tokens_reshaped.shape}")
                    
                    # å°è¯•è§£ç 
                    decoded_images = self.model.gen_vision_model.decode_code(
                        tokens_reshaped.to(dtype=torch.int)
                    )
                    logger.info(f"æ–¹æ³•{i}è§£ç æˆåŠŸ: {decoded_images.shape}")
                    return decoded_images
                    
                except Exception as e:
                    logger.debug(f"æ–¹æ³•{i}å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œåˆ›å»ºå ä½å›¾åƒ
            logger.warning("æ‰€æœ‰è§£ç æ–¹æ³•éƒ½å¤±è´¥ï¼Œåˆ›å»ºå ä½å›¾åƒ")
            img_size = 384  # é»˜è®¤å°ºå¯¸
            decoded_images = torch.zeros((batch_size, 3, img_size, img_size), 
                                       dtype=torch.float32, device=self.device)
            return decoded_images
            
        except Exception as e:
            logger.error(f"Tokenè§£ç å¤±è´¥: {e}")
            # è¿”å›å ä½å›¾åƒ
            img_size = 384
            return torch.zeros((batch_size, 3, img_size, img_size), 
                             dtype=torch.float32, device=self.device)
    
    def _create_test_image(self, batch_size, img_size):
        """åˆ›å»ºä¸€ä¸ªæœ‰å†…å®¹çš„æµ‹è¯•å›¾åƒè€Œä¸æ˜¯çº¯è‰²"""
        try:
            logger.info(f"åˆ›å»ºæµ‹è¯•å›¾åƒ: {batch_size}x{img_size}x{img_size}")
            
            # åˆ›å»ºä¸€ä¸ªæœ‰æ¸å˜å’Œå›¾æ¡ˆçš„æµ‹è¯•å›¾åƒ
            images = torch.zeros((batch_size, 3, img_size, img_size), dtype=torch.float32, device=self.device)
            
            for b in range(batch_size):
                # åˆ›å»ºæ¸å˜èƒŒæ™¯
                for i in range(img_size):
                    for j in range(img_size):
                        # çº¢è‰²é€šé“ï¼šæ°´å¹³æ¸å˜
                        images[b, 0, i, j] = j / img_size
                        # ç»¿è‰²é€šé“ï¼šå‚ç›´æ¸å˜
                        images[b, 1, i, j] = i / img_size
                        # è“è‰²é€šé“ï¼šå¾„å‘æ¸å˜
                        center_x, center_y = img_size // 2, img_size // 2
                        distance = ((i - center_x) ** 2 + (j - center_y) ** 2) ** 0.5
                        images[b, 2, i, j] = 1.0 - min(distance / (img_size // 2), 1.0)
                
                # æ·»åŠ ä¸€äº›å‡ ä½•å›¾æ¡ˆ
                # æ·»åŠ åœ†å½¢
                center_x, center_y = img_size // 2, img_size // 2
                radius = img_size // 4
                for i in range(max(0, center_x - radius), min(img_size, center_x + radius)):
                    for j in range(max(0, center_y - radius), min(img_size, center_y + radius)):
                        if (i - center_x) ** 2 + (j - center_y) ** 2 <= radius ** 2:
                            images[b, :, i, j] = torch.tensor([0.8, 0.2, 0.6])  # ç´«è‰²åœ†å½¢
                
                # æ·»åŠ çŸ©å½¢
                rect_size = img_size // 6
                rect_x, rect_y = img_size // 4, img_size // 4
                images[b, :, rect_x:rect_x+rect_size, rect_y:rect_y+rect_size] = torch.tensor([0.2, 0.8, 0.3]).view(3, 1, 1)  # ç»¿è‰²çŸ©å½¢
            
            # å°†å€¼èŒƒå›´è°ƒæ•´åˆ° [-1, 1]
            images = images * 2.0 - 1.0
            
            logger.info(f"æµ‹è¯•å›¾åƒåˆ›å»ºå®Œæˆ: {images.shape}, å€¼èŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
            return images
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæµ‹è¯•å›¾åƒå¤±è´¥: {e}")
            # è¿”å›ç®€å•çš„æ¸å˜å›¾åƒ
            images = torch.zeros((batch_size, 3, img_size, img_size), dtype=torch.float32, device=self.device)
            for b in range(batch_size):
                for i in range(img_size):
                    for j in range(img_size):
                        images[b, 0, i, j] = i / img_size * 2.0 - 1.0  # çº¢è‰²æ¸å˜
                        images[b, 1, i, j] = j / img_size * 2.0 - 1.0  # ç»¿è‰²æ¸å˜
                        images[b, 2, i, j] = 0.5  # è“è‰²å›ºå®šå€¼
            return images
